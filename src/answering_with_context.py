import ollama

from google import genai


def answer_with_context_local(question: str, context: str, system_prompt: str) -> str:
    """
    Answer a question based on the provided context using the local Ollama model.

    Args:
        question (str): The user's question.
        context (str): The context to use for answering.
        system_prompt (str): The system prompt for answering.

    Returns:
        str: The answer generated by the model.
    """
    prompt = system_prompt.format(context=context, question=question)
    r = ollama.generate(model="llama3:8b", prompt=prompt)
    return r['response'].strip()

def answer_with_context_cloud(question: str, context: str, system_prompt: str, client: genai.Client) -> str:    
    """
    Answer a question based on the provided context using the cloud-based model.

    Args:
        question (str): The user's question.
        context (str): The context to use for answering.
        system_prompt (str): The system prompt for answering.
        client: The cloud API client.

    Returns:
        str: The answer generated by the model.
    """
    prompt = system_prompt.format(context=context, question=question)
    r = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
    )
    return r.text.strip()

def combine_and_answer(question: str, summaries: dict, system_prompt: str, client: genai.Client = None) -> str:
    """
    Combine the summaries and answer the question using the appropriate model.

    Args:
        question (str): The user's question.
        summaries (dict): A dictionary mapping URLs to their summarized text.
        system_prompt (str): The system prompt for answering.
        client: The cloud API client (if using cloud answering).

    Returns:
        str: The final answer generated by the model.
    """
    context = "\n\n".join(summaries.values())
    if client is not None:
        return answer_with_context_cloud(question, context, system_prompt, client)
    return answer_with_context_local(question, context, system_prompt)
